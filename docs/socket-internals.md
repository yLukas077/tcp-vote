# TCP Sockets no Go

## 1. `net.Listen()` â†’ *3 syscalls fundamentais*

Quando vocÃª faz:

```
net.Listen("tcp", ":9000")
```

O Go chama no kernel:

1. **`socket()`**
   Cria um file descriptor TCP (ex: `fd=3`).

2. **`bind()`**
   Reserva a porta (ex: 9000).
   Se outra app estiver usando â†’ `EADDRINUSE`.

3. **`listen()`**
   Transforma o socket em modo de escuta e cria duas filas internas:

### ðŸ”¹ **SYN Queue (half-open)**

ConexÃµes que ainda nÃ£o concluÃ­ram o 3-way handshake.

### ðŸ”¹ **Accept Queue (estabelecidas)**

ConexÃµes prontas para o Go retirar com `Accept()`.
Se encher â†’ conexÃµes novas sÃ£o descartadas.

---

## 2. `Accept()` â†’ *Entrega um novo fd por cliente*

```
conn, _ := listener.Accept()
```

O Go chama:

### **`accept()`**

* Bloqueia se a Accept Queue estiver vazia.
* Remove a prÃ³xima conexÃ£o da fila.
* Cria **novo fd exclusivo para esse cliente** (ex: `fd=4`).
* Limite total: **ulimit** (ex: 1024 ou 65536 fds).

Cada cliente = 1 fd + 1 goroutine â†’ Go escala porque goroutines sÃ£o baratas.

---

## 3. `Write()` â†’ *Pode bloquear!*

```
conn.Write([]byte("msg"))
```

Chamado internamente:

### **`write()`**

Fluxo:

1. Copia dados para o **TCP send buffer** do kernel.
2. TCP fragmenta em MSS (~1460 bytes).
3. Controle de congestionamento decide quando enviar.

### ðŸ”¥ Por que pode bloquear?

Porque **TCP Ã© backpressure**:

1. Cliente para de ler.
2. TCP buffer do cliente enche â†’ manda janela zero.
3. Servidor nÃ£o pode enviar mais.
4. **Send buffer do servidor enche.**
5. `write()` **bloqueia** atÃ© liberar espaÃ§o.

Esse bloqueio pode durar **segundos**.

---

## 4. `Read()` com bufio â†’ *ReduÃ§Ã£o massiva de syscalls*

Sem `bufio`:

* Cada byte â†’ 1 syscall.
* 100 bytes â†’ 100 syscalls.

Com `bufio.NewReader`:

* Envia **1 syscall** para ler ~4KB.
* O resto Ã© leitura em RAM (nanosegundos).

EficiÃªncia cresce *ordens de magnitude*.

---

## 5. O Perigo Real: Mutex + Write Bloqueante

Se vocÃª fizer:

```
mu.Lock()
conn.Write()  // pode bloquear por segundos!
mu.Unlock()
```

E esse cliente estiver lento:

* A goroutine que segura o mutex **congela**.
* Todas as outras goroutines que precisam do mutex **travam tambÃ©m**.
* Nenhum voto Ã© processado.
* Servidor **para completamente**, mesmo com 1 cliente problemÃ¡tico.

Esse Ã© **o bug mais comum em servidores Go iniciantes**.

---

## 6. SoluÃ§Ã£o: Worker Async + Channels

Modelo correto:

### ðŸ”¹ AtualizaÃ§Ã£o de estado = rÃ¡pido

### ðŸ”¹ Broadcast = assÃ­ncrono em worker separado

```
mu.Lock()
atualiza memÃ³ria
copia snapshot
broadcastChan <- snapshot
mu.Unlock()
```

Worker:

```
for update := range broadcastChan {
    conn.Write(update)   // pode bloquear, mas fora da seÃ§Ã£o crÃ­tica
}
```

Resultado:

* Mutex fica travado por microssegundos.
* VotaÃ§Ãµes continuam mesmo com clientes lentos.
* Throughput dispara.

---

## 7. Por que o Go escala? (Goroutines vs Threads)

* Thread OS â†’ ~1â€“2 MB
* Goroutine â†’ ~2 KB
* Go runtime multiplexa milhares de goroutines em poucos threads SO.

Isso permite:

* 10.000 conexÃµes = **10.000 goroutines**
* Sem custo de thread OS
* Sem travar o kernel

---

# ðŸ“Œ O que realmente importa entender

### âœ” Syscalls criam filas internas no kernel (SYN queue e Accept queue).

### âœ” `Accept()` entrega 1 fd por cliente.

### âœ” `Write()` pode BLOQUEAR por segundos se o cliente nÃ£o ler.

### âœ” Nunca segure mutex durante operaÃ§Ãµes de rede.

### âœ” Use channels + workers para tornar o servidor imune a clientes lentos.

### âœ” bufio reduz drasticamente a quantidade de syscalls.

### âœ” Go escala usando goroutines muito mais leves que threads.