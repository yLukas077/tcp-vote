# TCP Sockets no Go

## 1. `net.Listen()` ‚Üí *3 syscalls fundamentais*

Quando voc√™ faz:

```
net.Listen("tcp", ":9000")
```

O Go chama no kernel:

1. **`socket()`**
   Cria um file descriptor TCP (ex: `fd=3`).

2. **`bind()`**
   Reserva a porta (ex: 9000).
   Se outra app estiver usando ‚Üí `EADDRINUSE`.

3. **`listen()`**
   Transforma o socket em modo de escuta e cria duas filas internas:

### üîπ **SYN Queue (half-open)**

Conex√µes que ainda n√£o conclu√≠ram o 3-way handshake.

### üîπ **Accept Queue (estabelecidas)**

Conex√µes prontas para o Go retirar com `Accept()`.
Se encher ‚Üí conex√µes novas s√£o descartadas.

---

## 2. `Accept()` ‚Üí *Entrega um novo fd por cliente*

```
conn, _ := listener.Accept()
```

O Go chama:

### **`accept()`**

* Bloqueia se a Accept Queue estiver vazia.
* Remove a pr√≥xima conex√£o da fila.
* Cria **novo fd exclusivo para esse cliente** (ex: `fd=4`).
* Limite total: **ulimit** (ex: 1024 ou 65536 fds).

Cada cliente = 1 fd + 1 goroutine ‚Üí Go escala porque goroutines s√£o baratas.

---

## 3. `Write()` ‚Üí *Pode bloquear!*

```
conn.Write([]byte("msg"))
```

Chamado internamente:

### **`write()`**

Fluxo:

1. Copia dados para o **TCP send buffer** do kernel.
2. TCP fragmenta em MSS (~1460 bytes).
3. Controle de congestionamento decide quando enviar.

### üî• Por que pode bloquear?

Porque **TCP √© backpressure**:

1. Cliente para de ler.
2. TCP buffer do cliente enche ‚Üí manda janela zero.
3. Servidor n√£o pode enviar mais.
4. **Send buffer do servidor enche.**
5. `write()` **bloqueia** at√© liberar espa√ßo.

Esse bloqueio pode durar **segundos**.

---

## 4. `Read()` com bufio ‚Üí *Redu√ß√£o massiva de syscalls*

Sem `bufio`:

* Cada byte ‚Üí 1 syscall.
* 100 bytes ‚Üí 100 syscalls.

Com `bufio.NewReader`:

* Envia **1 syscall** para ler ~4KB.
* O resto √© leitura em RAM (nanosegundos).

Efici√™ncia cresce *ordens de magnitude*.

---

## 5. O Problema Real: Mutex + Write Bloqueante

### ‚ùå Design Problem√°tico

```go
mu.Lock()
for _, conn := range clients {
    conn.Write(data)  // BLOQUEIA se buffer do cliente estiver cheio
}
mu.Unlock()  // Nunca executado durante bloqueio
```

**O que acontece:**

1. Cliente **para de chamar `read()`** (pode ser por qualquer motivo: CPU ocupada, rede congestionada, app pausado)
2. TCP receive buffer do cliente **enche** (t√≠pico: 128KB)
3. Servidor tenta enviar **256KB** (maior que buffer dispon√≠vel)
4. Kernel **retorna EAGAIN/EWOULDBLOCK** ‚Üí Go bloqueia a goroutine
5. Mutex **permanece travado** durante bloqueio
6. **TODAS as outras vota√ß√µes param** (precisam do mutex)

**Isso N√ÉO √© um ataque, √© TCP funcionando corretamente!**

### üéØ Por que o Teste Usa Payload de 256KB?

**Motivo t√©cnico:** Broadcasts pequenos (~30 bytes) levam **milhares de mensagens** para encher buffer TCP de 128KB.

**Solu√ß√£o para teste:** Enviar **256KB por broadcast** (maior que buffer):

```go
padding := strings.Repeat("\x00", 256*1024)  // 256KB
msg := fmt.Sprintf("UPDATE: %v | SNAPSHOT: %s\n", voteCounts, padding)
```

**Resultado:**
- **1¬™ ou 2¬™ mensagem** j√° excede capacidade do buffer
- `write()` **bloqueia imediatamente**
- Demonstra problema de design **rapidamente**

**Cen√°rios reais equivalentes:**
- Servidores de jogos: snapshots completos de estado (50-500KB)
- Sistemas de log: buffers agregados (100KB-1MB)
- APIs de streaming: chunks de v√≠deo/dados (256KB-2MB)

### ‚ö†Ô∏è Pr√©-requisito do Teste

Cliente precisa **votar primeiro** para entrar na lista de broadcast:

```go
fmt.Fprintf(conn, "VOTE A\n")  // Entra na lista
time.Sleep(‚àû)                   // Para de ler
```

Se n√£o votar, servidor nunca chama `conn.Write()` nele.

### ‚úÖ Solu√ß√£o Arquitetural

```go
mu.Lock()
snapshot := copy(state)  // Microssegundos
mu.Unlock()              // Liberado ANTES do I/O

broadcastChan <- snapshot  // Worker faz I/O separadamente
```

**Worker isolado:**
```go
for update := range broadcastChan {
    conn.Write(update)  // Pode bloquear, mas mutex j√° foi liberado
}
```

**Resultado:**
- Mutex travado por **< 100 microssegundos**
- Vota√ß√µes **continuam mesmo com buffers cheios**
- Worker bloqueia **isoladamente** sem afetar sistema

---

## 6. Solu√ß√£o: Worker Async + Channels

Modelo correto:

### üîπ Atualiza√ß√£o de estado = r√°pido

### üîπ Broadcast = ass√≠ncrono em worker separado

```
mu.Lock()
atualiza mem√≥ria
copia snapshot
broadcastChan <- snapshot
mu.Unlock()
```

Worker:

```
for update := range broadcastChan {
    conn.Write(update)   // pode bloquear, mas fora da se√ß√£o cr√≠tica
}
```

Resultado:

* Mutex fica travado por microssegundos.
* Vota√ß√µes continuam mesmo com clientes lentos.
* Throughput dispara.

---

## 7. Por que o Go escala? (Goroutines vs Threads)

* Thread OS ‚Üí ~1‚Äì2 MB
* Goroutine ‚Üí ~2 KB
* Go runtime multiplexa milhares de goroutines em poucos threads SO.

Isso permite:

* 10.000 conex√µes = **10.000 goroutines**
* Sem custo de thread OS
* Sem travar o kernel

---

# üìå O que realmente importa entender

### ‚úî Syscalls criam filas internas no kernel (SYN queue e Accept queue).

### ‚úî `Accept()` entrega 1 fd por cliente.

### ‚úî `Write()` pode BLOQUEAR por segundos se o cliente n√£o ler.

### ‚úî Nunca segure mutex durante opera√ß√µes de rede.

### ‚úî Use channels + workers para tornar o servidor imune a clientes lentos.

### ‚úî bufio reduz drasticamente a quantidade de syscalls.

### ‚úî Go escala usando goroutines muito mais leves que threads.